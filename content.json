{"meta":{"title":"CharlesBlog","subtitle":"","description":"","author":"Charles","url":"http://example.com","root":"/"},"pages":[{"title":"我的分类页","date":"2021-01-14T05:37:13.000Z","updated":"2021-01-14T05:37:52.981Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"music","date":"2021-06-09T06:14:15.000Z","updated":"2021-06-09T06:14:15.484Z","comments":true,"path":"music/index.html","permalink":"http://example.com/music/index.html","excerpt":"","text":""},{"title":"我的标签页","date":"2021-01-14T05:38:26.000Z","updated":"2021-01-14T05:39:00.600Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"我的小伙伴们","date":"2021-01-14T05:39:21.000Z","updated":"2021-01-14T05:39:55.004Z","comments":true,"path":"link/index.html","permalink":"http://example.com/link/index.html","excerpt":"","text":""},{"title":"","date":"2021-06-09T02:36:24.182Z","updated":"2021-06-09T02:36:24.181Z","comments":true,"path":"self/doutone.css","permalink":"http://example.com/self/doutone.css","excerpt":"","text":"/* Name: Duotone Light Author: Simurai, adapted from DuoTone themes for Atom (http://simurai.com/projects/2016/01/01/duotone-themes) Conversion: Bram de Haan (http://atelierbram.github.io/Base2Tone-prism/output/prism/prism-base2tone-morning-light.css) Generated with Base16 Builder (https://github.com/base16-builder/base16-builder) */ code[class*=\"language-\"], pre[class*=\"language-\"] { font-family: Consolas, Menlo, Monaco, \"Andale Mono WT\", \"Andale Mono\", \"Lucida Console\", \"Lucida Sans Typewriter\", \"DejaVu Sans Mono\", \"Bitstream Vera Sans Mono\", \"Liberation Mono\", \"Nimbus Mono L\", \"Courier New\", Courier, monospace; font-size: 14px; line-height: 1.375; direction: ltr; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; -moz-tab-size: 4; -o-tab-size: 4; tab-size: 4; -webkit-hyphens: none; -moz-hyphens: none; -ms-hyphens: none; hyphens: none; background: #faf8f5; color: #728fcb; } pre > code[class*=\"language-\"] { font-size: 1em; } pre[class*=\"language-\"]::-moz-selection, pre[class*=\"language-\"] ::-moz-selection, code[class*=\"language-\"]::-moz-selection, code[class*=\"language-\"] ::-moz-selection { text-shadow: none; background: #faf8f5; } pre[class*=\"language-\"]::selection, pre[class*=\"language-\"] ::selection, code[class*=\"language-\"]::selection, code[class*=\"language-\"] ::selection { text-shadow: none; background: #faf8f5; } /* Code blocks */ pre[class*=\"language-\"] { padding: 1em; margin: .5em 0; overflow: auto; } /* Inline code */ :not(pre) > code[class*=\"language-\"] { padding: .1em; border-radius: .3em; } .token.comment, .token.prolog, .token.doctype, .token.cdata { color: #b6ad9a; } .token.punctuation { color: #b6ad9a; } .token.namespace { opacity: .7; } .token.tag, .token.operator, .token.number { color: #063289; } .token.property, .token.function { color: #b29762; } .token.tag-id, .token.selector, .token.atrule-id { color: #2d2006; } code.language-javascript, .token.attr-name { color: #896724; } code.language-css, code.language-scss, .token.boolean, .token.string, .token.entity, .token.url, .language-css .token.string, .language-scss .token.string, .style .token.string, .token.attr-value, .token.keyword, .token.control, .token.directive, .token.unit, .token.statement, .token.regex, .token.atrule { color: #728fcb; } .token.placeholder, .token.variable { color: #93abdc; } .token.deleted { text-decoration: line-through; } .token.inserted { border-bottom: 1px dotted #2d2006; text-decoration: none; } .token.italic { font-style: italic; } .token.important, .token.bold { font-weight: bold; } .token.important { color: #896724; } .token.entity { cursor: help; } pre > code.highlight { outline: .4em solid #896724; outline-offset: .4em; } /* overrides color-values for the Line Numbers plugin * http://prismjs.com/plugins/line-numbers/ */ .line-numbers .line-numbers-rows { border-right-color: #ece8de; } .line-numbers-rows > span:before { color: #cdc4b1; } /* overrides color-values for the Line Highlight plugin * http://prismjs.com/plugins/line-highlight/ */ .line-highlight { background: rgba(45, 32, 6, 0.2); background: -webkit-linear-gradient(left, rgba(45, 32, 6, 0.2) 70%, rgba(45, 32, 6, 0)); background: linear-gradient(to right, rgba(45, 32, 6, 0.2) 70%, rgba(45, 32, 6, 0)); } :root { --hl-color: #728fcb; --hl-bg: #faf8f5; --hltools-bg: xxxxxxx; --hltools-color: xxxxxxx; --hlnumber-bg: xxxxxxx; --hlnumber-color: xxxxxxxx; --hlscrollbar-bg: xxxxx; --hlexpand-bg: xxxxxxx } code[class*=\"language-\"], pre[class*=\"language-\"] { font-family: Consolas, Menlo, Monaco, \"Andale Mono WT\", \"Andale Mono\", \"Lucida Console\", \"Lucida Sans Typewriter\", \"DejaVu Sans Mono\", \"Bitstream Vera Sans Mono\", \"Liberation Mono\", \"Nimbus Mono L\", \"Courier New\", Courier, monospace; font-size: 14px; line-height: 1.375; direction: ltr; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; -moz-tab-size: 4; -o-tab-size: 4; tab-size: 4; -webkit-hyphens: none; -moz-hyphens: none; -ms-hyphens: none; hyphens: none; background: #faf8f5; color: #728fcb; } pre > code[class*=\"language-\"] { font-size: 1em; } pre[class*=\"language-\"]::-moz-selection, pre[class*=\"language-\"] ::-moz-selection, code[class*=\"language-\"]::-moz-selection, code[class*=\"language-\"] ::-moz-selection { text-shadow: none; background: #faf8f5; } pre[class*=\"language-\"]::selection, pre[class*=\"language-\"] ::selection, code[class*=\"language-\"]::selection, code[class*=\"language-\"] ::selection { text-shadow: none; background: #faf8f5; } /* Code blocks */ pre[class*=\"language-\"] { padding: 1em; margin: .5em 0; overflow: auto; } /* Inline code */ :not(pre) > code[class*=\"language-\"] { padding: .1em; border-radius: .3em; } /* ------------------------------------- */ /* 到这里为止，可以删除 */ .token.comment, .token.prolog, .token.doctype, .token.cdata { color: #b6ad9a; } .token.punctuation { color: #b6ad9a; } .token.namespace { opacity: .7; } .token.tag, .token.operator, .token.number { color: #063289; } .token.property, .token.function { color: #b29762; } .token.tag-id, .token.selector, .token.atrule-id { color: #2d2006; } code.language-javascript, .token.attr-name { color: #896724; } code.language-css, code.language-scss, .token.boolean, .token.string, .token.entity, .token.url, .language-css .token.string, .language-scss .token.string, .style .token.string, .token.attr-value, .token.keyword, .token.control, .token.directive, .token.unit, .token.statement, .token.regex, .token.atrule { color: #728fcb; } .token.placeholder, .token.variable { color: #93abdc; } .token.deleted { text-decoration: line-through; } .token.inserted { border-bottom: 1px dotted #2d2006; text-decoration: none; } .token.italic { font-style: italic; } .token.important, .token.bold { font-weight: bold; } .token.important { color: #896724; } .token.entity { cursor: help; } pre > code.highlight { outline: .4em solid #896724; outline-offset: .4em; } /* overrides color-values for the Line Numbers plugin * http://prismjs.com/plugins/line-numbers/ */ .line-numbers .line-numbers-rows { border-right-color: #ece8de; } .line-numbers-rows > span:before { color: #cdc4b1; } /* overrides color-values for the Line Highlight plugin * http://prismjs.com/plugins/line-highlight/ */ .line-highlight { background: rgba(45, 32, 6, 0.2); background: -webkit-linear-gradient(left, rgba(45, 32, 6, 0.2) 70%, rgba(45, 32, 6, 0)); background: linear-gradient(to right, rgba(45, 32, 6, 0.2) 70%, rgba(45, 32, 6, 0)); }"}],"posts":[{"title":"深度学习基础","slug":"深度学习基础","date":"2021-11-04T02:21:00.000Z","updated":"2021-11-04T13:28:02.246Z","comments":true,"path":"2021/11/04/深度学习基础/","link":"","permalink":"http://example.com/2021/11/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","excerpt":"","text":"本节目录如下： 前言 监督学习与无监督学习 神经网络 损失函数 梯度下降前言 人工智能可以归结于一句话：针对特定的任务，找出合适的数学表达式，然后一直优化表达式，直到这个表达式可以用来预测未来。 接下来就一句一句的分析这句话： 针对特定的任务： 首先我们需要知道的是，人工智能其实就是为了让计算机看起来像人一样智能，为什么这么说呢？举一个人工智能的例子： 我们人可看到一个动物的图片，就可以立刻知道这个动物是猫，还是狗。但是计算机却不可以，如果这个计算机可以分出类别，那么这就会使一个具有图像分类功能的人工智能的小例子。 这里的图像分类就是我们所说的特定任务，这就是我们希望写出一个人工智能的程序来做的事情。 还有一些其他的常见的任务：人脸识别，目标检测，图像分割，自然语言处理等等。 找出合适的数学表达式 学过高等数学并且有计算机思维的人都知道，世界中几乎所有的事情都可以用数学函数来表达出来，我们先不管这个数学表达式是离散还是连续，也不管它的次数多高，反正它都能达到表示特定任务的一种目的。 比如说，针对一个西瓜质量的好坏的预测任务，可以设出一下的表达式： 解释如下： 1、x1,x2,x3可以看作判断西瓜好坏的判断依据，比如说可以是：瓜皮纹路，敲击声音，瓜皮颜色等等。2、a,b,c,d就是这个表达式的系数，一旦数学表达式定下来了，那么接下来需要做的事情就是找出合适的系数，使得这个表达式可以很好的判断出西瓜质量的好坏。 所以，针对上文提到的特定任务，都可以用数学表达式表示出来，当然，我们会尽可能找出简单、高效的表达式。 一直优化这个表达式： 上边引出表达式之后，会发现当表达式确定下来之后，就要寻找合适的系数了,寻找系数的过程就被称为训练网络的过程。 我们优化表达式的重要思想是：一直调整系数值，使得预测出来的数据与真实数据之间的差距尽可能的最小。 比如：假设预测的数据是f1(x)，真实数据是y，我们通过一直改变系数的值，来找出可以使得预测数据与真实数据之间距离最小的一组，最小的一组数据就是我们需要的系数。 其中，距离计算公式可以是如下的表达式： 通过这个表达式，得到的loss值就是真实值与预测值之间的距离。 然后，接下来的优化就是针对这个loss表达式来进行的，目的就是让loss的值达到最小。 因为loss值达到最小的时候，就意味着我们的预测值与真实值距离很相近，预测越准确。 这里值得一提的是，这里的loss表达式的优化过程，其实就是将loss公式对函数f(x)的系数的求导。所以当loss最小的时候，就意味着此时的系数最合适。具体的细节往下看。 用优化好的表达式预测未来：经过上边的优化，此时函数会得到一个相对好一点的系数，然后就可以使用这个函数来预测未来的事情了。 这就是达到了人工智能的目的了。 所以，下边我们就要仔细讨论，数学表达式的构建，距离函数的构建，距离的优化。 神经网络 神经网络的英文是：neural network（简称NN）。 神经网络其实就是变形的数学表达式，它通过拼装基础组件（神经元）来模拟出数学表达式。 什么是神经网络一说神经网络，大家首先想到的就是神经元，其实没错，神经网络这个名词就是从神经元这里演变过来的。所以我们做一下类比。 神经元 如图所示，这个图就是我们人体的神经元的大图。 通常我们身体的A部位发出的命令，要指挥B部位响应，就要通过A向B发出信号。这个信号的强弱影响着B反应的强弱。 所以，这就是神经网络的构思所在： 构建出一个类似于神经元的结构，上一个节点的输入（A处的控制） 以及权重（信号的强弱）共同决定下一个节点的输出（B处的反应）。 这句话，现在看不懂没关系，有个印象就好，继续往下看吧。 神经网络 如图所示就是一个最简单的神经网络结构，这个结构的数学表达式是： 图中的圆圈我们就把他类比于神经元，图中的各个结构解释如下： 其中 X1，X2 就是这个神经网络的输入，他相当于就是人体大脑发出的控制命令。 W1，W2 就是权重，他是用来控制不同输入信号占比大小的数据，比如：想让控制X1作用明显一点，那么对应的W1就大一点。 Y就是输出，他就是输入数据与权重作用之后的最终结果，在神经元中也就是最终对身体某个部位的控制信号。 神经网络的数学原理神经网络的数学原理非常简单，简单总结下来就是一句话：不同的输入作用于各自的权重之后的和即为我们需要的结果。 其实就可以大致理解为我们的函数：f(x) = ax1 + bx2 一样，所谓的权重就是我们方程的系数。 细心的人观察上边的公式就会发现，一个神经元节点就可以归结于一个运算式子。所以我们这里就来针对上图，分析分析含有一个神经元节点的公式。 从图中可以看得出来，最终的输出结果Y是由 输入（X） 以及 权重（W） 共同决定的。 他们最终的计算结果 Y其实说白了就是一个计算公式：Y = X1W1 + X2W2​ ，这个公式的含义大家应该都明白，给不同的输入 分配不同的权重 ，从而得到想要的结果。 这就是神经网络中一个神经元的数学原理，当把神经元的个数增多之后，原理以此类推，只不过是要增加权重W以及输入X的个数而已。 下边就可以看作是一个，含有两层的神经网络结构。 第一层节点： 11，12，13 。第二层节点： 21 。 输入： X1 ，X2 。 输出 ：Y 。遇是，根据公式：输出等于输入作用于权重，得出以下推导： 输入：X1，X2. 节点11的值： 节点12的值： 节点13的值： 节点21的值就是最终输出Y： 所以，最终的整合式子为： 于是，我们可以发现，类似于这样的堆叠方式，我们可以组合成很多的数学函数。 这就是神经网络，他的目的在于将数学公式堆砌出来，至于为什么要这样堆砌，是因为这样堆砌计算机计算比较方便呗。 总结到目前为止你已经知道了神经网络的由来，并且知道神经网络与数学公式之间的关系。 此时你需要明确的知识点是： 人工智能就是使用已有的数据，拟合出一个可以用来预测未来的公式。 这个公式的系数需要一直调整，从而找出一组最为合适，正确率较高的系数。 因为系数的寻找需要大量的计算，所以需要将这个公式用神经网络表示出来的，因为在计算机中这样表示的时候计算最为方便。 监督学习与无监督学习 这个知识点比较简单，就一些单纯的概念。 监督学习： 就是我们收集到的数据是有标签的。 就是说，我们收集到的数据是已经分好类的。比如说：当前当前有一批样本数据， x1, x2, x6, x9, x13属于类别y1类。 x3, x4, x5, x8, x11属于类别y2类。 x7, x10, x12属于类别y3类。 然后接下来我们使用这些数据的时候，就可以使用已有标签的数据，去拟合出曲线，用以预测未来。 无监督学习： 我们收集到的数据是无标签的。 就是说，收集到的数据并没有固定的类别，我们需要做的事情就是挖掘数据内部的联系，给他们聚类，找出类别。 如图所示，挖掘出数据内部的联系，让他自动归类。 损失函数 上边解释过了，损失函数的作用就是计算 真实值 与 预测值 之间距离的 （距离其实可以简单理解为两个数据之间的差距）。 这里介绍一些常见的几种损失函数，以供大家入门使用。 一些前提 这里给定一些大前提，下边的几种损失函数通用的那种。 真实值：y ，他就是针对某一组输入x的真实标签。 预测值：f(x)，他就是针对输入x的预测标签。 样本数：m，他就是我们每次输入多少样本进行计算，比如：某一次输入5组x，得到5个预测结果，这里的m=5. 绝对值损失函数 其实就是简单的计算 真实值 与 预测值 之间的绝对值距离而已。 公式： 解释： J(y,f(x)) 的意思就是，这个损失函数的参数是：真是标签y 与 预测数据f(x) 。 J(w,b) 的意思是，这个损失函数的目的是优化参数 w 与 b 。这里的w ，b 其实就是系数的矩阵形式。 后边具体的计算公式就是：输入有m 个样本，计算出这m 个样本的距离绝对值和，然后再求均值。 均方差损失函数 就是将上边式子的绝对值换成平方就好了。 公式： 解释： 这里只是将绝对值换成了平方，除以m换成了除以2m。 交叉熵损失函数 这个就比较麻烦了，交叉熵损失函数一般用于解决分类问题。 标签： 在通常的分类问题中，标签y的取值一般只有 0 或 1 。 1 表示是当前类别， 0 表示不是当前类别。 公式： 解释： 上边说了，y与 f(x) 都只能取 1与 0f 中的一种可能性。所以，上述公式的效果就是： 如果 y与 f(x) 相同，则 J = 0. 你带入y=1 , f(x)=1 试试就知道了。 总结到这里你已经学习了三种常见的损失函数。 此时你应该有一个明确的知识点就是： 损失函数是用来计算真实值与预测值之间距离的。 当损失函数的值越小就代表着真实值与预测值之间的距离就越小，也就意味着预测的越准。 梯度下降 好了好了，上边过完理论知识，这里来一个真真正正的数学内容了，其实不难，看我慢慢分析。 上边我们提到对数学函数优化的时候，只是介绍了理论的知识。 我们知道了损失函数就是衡量预测值与真实值之间距离的公式。并且知道，损失函数的值越小，真实值和预测值之间的距离越小，也即：预测的越准。 但是并没有带着大家深入探究如何优化。 也就是没有告诉大家怎么使得损失函数的值越来越小。 其实，这里使用的数学知识就是 ：求偏导 数学例子 这里以一个简单的数学例子来引入梯度下降的内容。 场景引入 在数学课中我们经常做的一个题型就是：已知一个函数f(x)的表达式，如何求出这个式子的最小值点。 在数学题中我们经常用的方法就是：将函数f(x)对x求导，然后令导数式子为0，求出此时的x的值，即为最小值点的位置。 具体例子 求函数的最小值点，并且求出最小值。 对函数求导 令导函数为0，求出此时的x 此时，x=3即为函数 f(x)的最小值点，带入原方程 f(3)= 29-123+20 = 2. 这个解题过程，想必大家都很熟悉吧。 下边就分析一下这个过程的数学原理了 数学例子原理 梯度就是导数。 针对上边提到的方程的最小值求解，其实就是求出其梯度（导数）为0的位置，就是其最低点的位置。具体看下图： 方程图像如下： 从图中可以看出，方程在不同位置的导数方向是不同的，只有在最低点的位置，导数为0，所以可以用导数为0的位置求出最低点。 上边举的例子是一个比较简单的例子，方程中只有一个未知数，但是在真实情况中，往往一个方程有很多未知数。 比如： 此时需要做的事情就是针对每一个变量求偏导，求出该方程针对每个变量的梯度方向 （梯度方向就是数据变小的方向）。 于是，在方程的每个点上，都有多个梯度方向，最终将这多个方向合并，形成这个点的最终梯度方向 （数据变小的方向） 如图，方程有两个变量x,y，于是在A点针对两个变量求偏导就可以得到各自的梯度方向（两个红色箭头的方向）。然后，将两个梯度进行合并，得到最终的梯度方向Z 。Z方向就是方程在A点数据变小的方向了。 完整例子 上边讲完原理，这里就举出一个例子，带着大家走一遍梯度下降找最小值的过程。 假设此时的方程已知，并且根据方程绘制出的图像如下。 刚开始我们位于A点： 1、在A点处针对方程的各个变量求出偏导，于是便可以得到方程针对各个方向的梯度方向。2、将A点处各个方向的梯度方向进行合并，形成最终的梯度方向。3、最终的梯度方向就是AB方向。4、于是向着AB方向走出一段距离，走到了B点。 到达B点： （思路同上） 1、求出B点处各个方向的梯度方向，然后合并所有梯度方向，得到最终的B点处梯度方向 BC。2、于是沿着BC方向，走出一段距离，到达C点。 …重复上述过程： 到达某个点之后，求出各方向的偏导数，然后合并得到最终的梯度方向。然后沿着合并后的梯度方向走出一段距离到达下一个点。然后在一直重复…… 到达K点： K点就是最终的点，这就是优化得到的最重点。 这就是整个找最小点的可视化过程，但是其中提到更新的数学细节并没有提到，所以下边提一下用到的数学更新公式吧 更新公式 一般我们梯度下降更新的数据只有函数的系数，然后函数的系数可以分为两类：权重（W）+ 偏差（b）所以，更新的时候也就针对这两个参数就好了。 变量定义： w ： 方程的权重。 （可以简单理解为方程变量前面的系数） b ：方程的偏差。 （可以简单理解为方程中的常数） 比如：中，2 , 1 就是权重，3就是偏差。 公式： 更新权重W ： 原始点的权重是 ，原始点此时针对W 的梯度方向是.α 就是一段距离长度（它就是我们上文一直提到的走一段距离）。所以 表达的含义就是沿着** W** 的梯度走一段长度为 α 的距离。然后 新的W 就是 旧的W 减去那一段方向长度。 更新偏差： 原理同W. 这就是更新参数的整个梯度下降过程了。 总结到目前为止，基础的人工智能知识已经基本讲完了，这个时候我们再来仔细品味这句话。 针对特定的任务，找出合适的数学表达式，然后一直优化表达式，直到这个表达式可以用来预测未来。 或许你就会有不一样的体会了。 参考文献：https://www.cnblogs.com/xiaoxiaojiea/p/14637326.html","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"进入深度学习","slug":"进入深度学习","permalink":"http://example.com/tags/%E8%BF%9B%E5%85%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"author":"Charles"},{"title":"特征工程-特征降维","slug":"特征工程-特征降维","date":"2021-09-23T13:52:00.000Z","updated":"2021-09-23T13:56:09.187Z","comments":true,"path":"2021/09/23/特征工程-特征降维/","link":"","permalink":"http://example.com/2021/09/23/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/","excerpt":"","text":"","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程-特征降维","slug":"特征工程-特征降维","permalink":"http://example.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"}],"author":"Charles"},{"title":"特征工程-特征预处理","slug":"特征工程-特征预处理","date":"2021-09-20T13:12:00.000Z","updated":"2021-09-20T13:13:57.202Z","comments":true,"path":"2021/09/20/特征工程-特征预处理/","link":"","permalink":"http://example.com/2021/09/20/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/","excerpt":"","text":"什么是特征预处理特征预处理定义 scikit-learn的解释provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. 翻译过来：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程 为什么我们要进行归一化/标准化？ 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些算法无法学习到其它的特征 举例：约会对象数据 我们需要用到一些方法进行无量纲化，使不同规格的数据转换到同一规格. 包含内容(数值型数据的无量纲化) 归一化 标准化特征处理API sklearn.preprocessing 归一化定义通过对原始数据进行变换把数据映射到(默认为[0,1])之间 公式 作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0 API sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… ) MinMaxScalar.fit_transform(X)X:numpy array格式的数据[n_samples,n_features] 返回值：转换后的形状相同的array 数据计算我们对以下数据进行运算，保存的就是之前的约会对象数据1234540920,8.326976,0.953952,314488,7.153469,1.673904,226052,1.441871,0.805124,175136,13.147394,0.428964,138344,1.669788,0.134296,1 分析 实例化MinMaxScalar 通过fit_transform转换pandas as pd12345678910111213141516from sklearn.preprocessing import MinMaxScalerdef minmax_demo(): &quot;&quot;&quot; 归一化演示 :return: None &quot;&quot;&quot; data &#x3D; pd.read_csv(&quot;.&#x2F;data&#x2F;dating.txt&quot;) print(data) # 1、实例化一个转换器类 transfer &#x3D; MinMaxScaler(feature_range&#x3D;(2, 3)) # 2-3之间 # 2、调用fit_transform data &#x3D; transfer.fit_transform(data[[&#39;milage&#39;,&#39;Liters&#39;,&#39;Consumtime&#39;]]) print(&quot;最小值最大值归一化处理的结果：\\n&quot;, data) return None 返回结果Liters Consumtime target1234567891011121314151617180 40920 8.326976 0.953952 31 14488 7.153469 1.673904 22 26052 1.441871 0.805124 13 75136 13.147394 0.428964 1.. ... ... ... ...998 48111 9.134528 0.728045 3999 43757 7.882601 1.332446 3[1000 rows x 4 columns]最小值最大值归一化处理的结果： [[ 2.44832535 2.39805139 2.56233353] [ 2.15873259 2.34195467 2.98724416] [ 2.28542943 2.06892523 2.47449629] ..., [ 2.29115949 2.50910294 2.51079493] [ 2.52711097 2.43665451 2.4290048 ] [ 2.47940793 2.3768091 2.78571804]] 归一化总结因为最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差（健壮性），只适合传统精确小数据场景。怎么办？ 标准化定义通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内 公式 作用于每一列，mean为平均值，σ为标准差 所以回到刚才异常点的地方，我们再来看看标准化 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。 API sklearn.preprocessing.StandardScaler( ) 处理之后每列来说所有数据都聚集在均值0附近标准差差为1 StandardScaler.fit_transform(X) X:numpy array格式的数据[n_samples,n_features] 返回值：转换后的形状相同的array 数据计算同样对上面的数据进行处理 分析 实例化StandardScaler 通过fit_transform转换1234567891011121314151617181920import pandas as pdfrom sklearn.preprocessing import StandardScalerdef stand_demo(): &quot;&quot;&quot; 标准化演示 :return: None &quot;&quot;&quot; data &#x3D; pd.read_csv(&quot;dating.txt&quot;) print(data) # 1、实例化一个转换器类 transfer &#x3D; StandardScaler() # 2、调用fit_transform data &#x3D; transfer.fit_transform(data[[&#39;milage&#39;,&#39;Liters&#39;,&#39;Consumtime&#39;]]) print(&quot;标准化的结果:\\n&quot;, data) print(&quot;每一列特征的平均值：\\n&quot;, transfer.mean_) print(&quot;每一列特征的方差：\\n&quot;, transfer.var_) return None 返回结果：1234567891011121314151617181920212223 milage Liters Consumtime target0 40920 8.326976 0.953952 31 14488 7.153469 1.673904 22 26052 1.441871 0.805124 1.. ... ... ... ...997 26575 10.650102 0.866627 3998 48111 9.134528 0.728045 3999 43757 7.882601 1.332446 3[1000 rows x 4 columns]标准化的结果: [[ 0.33193158 0.41660188 0.24523407] [-0.87247784 0.13992897 1.69385734] [-0.34554872 -1.20667094 -0.05422437] ..., [-0.32171752 0.96431572 0.06952649] [ 0.65959911 0.60699509 -0.20931587] [ 0.46120328 0.31183342 1.00680598]]每一列特征的平均值： [ 3.36354210e+04 6.55996083e+00 8.32072997e-01]每一列特征的方差： [ 4.81628039e+08 1.79902874e+01 2.46999554e-01] 标准化总结在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"特征工程-特征预处理","slug":"特征工程-特征预处理","permalink":"http://example.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/"}],"author":"Charles"},{"title":"数据处理与特征工程-特征提取","slug":"进入机器学习","date":"2021-06-09T03:46:51.000Z","updated":"2021-09-20T13:13:54.345Z","comments":true,"path":"2021/06/09/进入机器学习/","link":"","permalink":"http://example.com/2021/06/09/%E8%BF%9B%E5%85%A5%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"console.error(\"Error: [hexo-tag-aplayer] Meting support is disabled, cannot resolve the meting tags properly.\"); 数据集可用数据集 公司内部、百度 数据接口（￥） 各种数据集 学习阶段可用的数据集 sklean kaggle UCIsklearn 数据集 导入数据集和工具库 123#导入sklearn数据包from sklearn.datasets import load_iris #导入鸢尾花数据集from sklearn.model_selection import train_test_split #导入模型划分 数据查看 1234567891011#字典格式total&#x3D;load_iris()#print(total)#数据目标值print(total.target_names)print(total.target)#数据内容（数据特征值）print(total.feature_names)print(total.data)#数据描述print(total.DESCR) 对数据集进行划分 1234#返回值 训练集 和 测试集x_train,x_test,y_train,y_test&#x3D;train_test_split(total.data,total.target,test_size&#x3D;0.24,random_state&#x3D;25)print(&quot;训练集特征值及目标值：\\n&quot;,x_train,&quot;\\n&quot;,y_train)print(&quot;测试集特征值及目标值：\\n&quot;,x_test,x_train) 特征工程 为什么需要特征工程 数据和特征决定了机器学习的上限，而模型和算法知识逼近这个上限而已 什么是特征工程 pandas ——&gt; 数据清洗、数据处理sklearn ——&gt; 特征工程 特征提取定义将任意数据（如文本或图像）转化为可用于机器学习的数字特征 注：特征值化是为了计算机更好的理解数据sklearn.feature_extraction（特征提取API） 特征提取的分类 字典特征提取（特征离散化） 文本特征提取 图像特征提取（深度学习） 字典特征提取 字典特征提取 ——&gt; 类型 ——&gt; one-hot编码 导入sklearn数据包123from sklearn.datasets import load_iris #导入鸢尾花数据集from sklearn.model_selection import train_test_split #导入模型划分from sklearn.feature_extraction import DictVectorizer #导入字典特征转换器 利用转化器进行特征提取123456789data&#x3D;[&#123;&#39;city&#39;:&#39;北京&#39;,&#39;temperature&#39;:100&#125;, &#123;&#39;city&#39;:&#39;上海&#39;,&#39;temperature&#39;:60&#125;, &#123;&#39;city&#39;:&#39;深圳&#39;,&#39;temperature&#39;:30&#125;]#1.实例化一个转化器类transfer&#x3D;DictVectorizer(sparse&#x3D;False)#sparse&#x3D;Ture 返回sparse矩阵（稀疏矩阵将非零值按位置返回）---&gt;可以提高效率#2.调用fit_transform()data_new&#x3D;transfer.fit_transform(data)print(transfer.get_feature_names())print(data_new) 中文文本特征提取 方法一：CountVectorizer统计每个样本特征词出现的个数 stop_words停用词 停用词表 1234567891011121314151617181920212223242526272829from sklearn.feature_extraction.text import CountVectorizer#导入文本特征转换器data&#x3D;[&quot;人生苦短我要学习深度学习，加油奥里给我里宝贝。&quot;, &quot;菏泽曹县666窝里宝贝。&quot;, &quot;我爱你桑桑。&quot;]data1&#x3D;&quot;我爱北京天安门&quot;#使用jieba切割文档（切割参数为字符串）&#39;&#39;&#39;a&#x3D;list(jieba.cut(data1))b&#x3D;&quot; &quot;.join(list(jieba.cut(data1)))print(a)print(type(a))print(b)print(type(b))&#39;&#39;&#39;#1.将中文文本进行分词data_new&#x3D;[]for sent in data: #a&#x3D;&quot; &quot;.join(list(jieba.cut(sent))) #print(a) data_new.append(&quot; &quot;.join(list(jieba.cut(sent))))print(data_new)#2.实现一个转化器类transfer&#x3D;CountVectorizer(stop_words&#x3D;[&quot;666&quot;,&quot;宝贝啊&quot;])#3.调用fit_transformdata_fin&#x3D;transfer.fit_transform(data_new)print(&quot;特征名字：\\n&quot;,transfer.get_feature_names())print(&quot;data_fin：\\n&quot;,data_fin.toarray()) 方法二：TfidfVectorize区别于方法一：更容易找到关键词（在其他文章出现很少，在本文中出现很多的词汇） TF —— 词频（term frequency） IDF —— 由总文件数目除以包括该词语之文件的数目，再到的商取以10为底的对数得到 TF-IDF=TF×IDF —— 重要程度 12345678910111213141516from sklearn.feature_extraction.text import TfidfVectorizerdata&#x3D;[&quot;人生苦短我要学习深度学习，加油奥里给我里宝贝。&quot;, &quot;菏泽曹县666窝里宝贝。&quot;, &quot;我爱你桑桑。&quot;]#1.将文本进行分词data_new&#x3D;[]for sent in data: data_new.append(&quot; &quot;.join(list(jieba.cut(sent))))#2.实例化一个转换器对象transfer&#x3D;TfidfVectorizer()#3.利用转换器对象进行数据分割data_fin&#x3D;transfer.fit_transform(data_new)print(&quot;特征名字：\\n&quot;,transfer.get_feature_names())print(&quot;data_fin：\\n&quot;,data_fin.toarray())","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"数据处理与特征工程-特征提取","slug":"数据处理与特征工程-特征提取","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"}],"author":"Charles"},{"title":"了解机器学习","slug":"了解机器学习","date":"2021-06-08T12:11:51.000Z","updated":"2021-06-09T03:29:19.182Z","comments":true,"path":"2021/06/08/了解机器学习/","link":"","permalink":"http://example.com/2021/06/08/%E4%BA%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"人工智能概述人工智能概述 达特茅斯会议-人工智能的起点 机器学习是人工智能的一个实现途径 深度学习是机器学习的一个方法发展而来机器学习、深度学习能做些什么 传统预测 图像识别 自然语言处理 什么是机器学习机器学习定义机器学习是从数据中自动分析获得模型，并利用模型对未知数据进行预测 解释类似于人 机器学习数据集结构：特征值+目标值 机器学习算法分类监督学习 目标值：类别 —— 分类问题 具体算法：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归 目标值：连续性数据 —— 回归问题 具体算法：线性回归、岭回归 无监督学习 目标值：无 具体算法：k-means 练习 1.预测明天的气温是多少度？ 回归 2.预测明天是阴、晴还是雨？ 分类 3.人脸年龄预测？ 分类（老小）/回归（具体年龄） 4.人脸识别？ 分类 机器学习开发流程 获取数据 数据处理 特征工程 建立模型（算法训练） 模型评估 应用 学习框架和资料介绍 算法是核心、数据与计算是基础 找准定位 大部分复杂模型的算法设计都是算法工程师在做，而我们做的是： 分析很多数据 分析具体的业务 应用常见的算法 特征工程、调参、优化 怎么做？ 入门 实战类书记 机器学习（西瓜书） - 周志华统计学习方法 - 李航深度学习（花书）","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"了解机器学习","slug":"了解机器学习","permalink":"http://example.com/tags/%E4%BA%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"author":"Charles"},{"title":"hexo博客搭建与优化","slug":"hexo博客搭建","date":"2021-01-14T07:32:42.000Z","updated":"2021-06-09T03:29:50.720Z","comments":true,"path":"2021/01/14/hexo博客搭建/","link":"","permalink":"http://example.com/2021/01/14/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","excerpt":"","text":"安装需要的软件软件安装 我们需要安装nodejs、git bash、npm、Hexo其中我们只需要下载前两个就行了 Nodejs下载地址 Git下载地址 Git安装参考 Git配置参考 最新版的Nodejs中包含了npm，所以我们不需要额外的安装了 下载完之后无脑点下一步就可以了,哈哈是不是很简单呢？ 我们打开Git Bash输入下面命令来检测安装是否成功 node -v和npm -v如果没有问题的话，应该是这样字的。 我们启动cmd，输入以下两条命令，成功后之后通过npm全局安装的包都会存放到node_global文件夹(这个文件夹是我自己创建的,你也可以创建在不同位置)下，后续查找包较方便。 123npm config set prefix &quot;D:\\Program Files\\nodejs\\node_global&quot;npm config set cache &quot;D:\\Program Files\\nodejs\\node_cache&quot; 这里的路径要记住，接下来可能我们在配置环境变量的时候，还将会用到 检测没问题之后，我们安装最后一个东西hexo打开Git Bash输入npm install -g hexo安装完之后的显示信息如下所示: 但是如果你会发现Hexo无法使用,我们还需要配置环境变量 配置环境变量这步，可能会比较的难，你需要跟着我做。 我的电脑右击属性-&gt;点高级系统配置-&gt;点环境变量-&gt;在最上边一栏双击Path-&gt;点击新建-&gt;然后输入你node_global的路径 步骤如图所示: 开始秒建博客准备工作做了这么多了，现在我们终于要开始搭建自己的博客了,进入一个你博客存放的位置,然后在命令行中输入hexo init,这是博客的初始化操作,紧接着我们休息片刻喝口水.现在初始化完毕了，我们首先看看我们的博客长什么样子吧!我们在命令行中输入hexo s(s是start的意思),然后我们打开浏览器输入网址localhost:4000我们就可以看到自己的博客了，博客如下图所示:这时候大家可能会说，这是什么垃圾博客，白给我都不要，走了走了，算了算了。大哥们，咱们先简单配置一下并说说Hexo的基本用法，然后就美化一下，相信我，最后不会很垃圾的. 简单配置一下博客博客的配置文件是你博客的根目录的_config.yml文件然后我们打开文件,进行基本的配置 12345678# Sitetitle: CharlesBlog # 这里写博客的名字subtitle: # 这里写博客的副标题description: &#39;&#39; # 关于你博客的描述keywords:author: Charles # 这里写博客的作者language: zh-CN # 博客语言timezone: &#39;&#39; 简单配置完毕，接来下我们学一下Hexo的基本命令 Hexo基本命令 命令 作用 Hexo init 初始化博客 Hexo s 运行博客 Hexo n title 创建一篇新的文章，文章标题是title Hexo c 清理文件 Hexo g 生成静态文件 Hexo d 程序部署博客（需要插件） 安装主题并简单配置butterfly主题 Hexo官网主题 主题的预览图: 接下来执行git clone https://github.com/jerryc127/hexo-theme-butterfly themes/butterfly进行安装,这里我们先让他安装着，我们打开我们的配置文件找到这一行theme: landspace然后将landspace替换成我们的主题也就是butterfly 此时运行博客 是运行不了的，因为我们少了一个插件，我们通过命令npm install hexo-renderer-pug hexo-renderer-stylus来安装这个插件最终完美运行. 为了以后升级方便，这里不推荐直接对主题的配置文件进行修改，而是复制配置文件进行修改。个人推荐把主題的配置文件_config.yml复制到 Hexo 的根目录下命名为_config.butterfly.yml，如果目录不存在那就创建一个。 配置主页大图片安装完博客后我们看到的就是这个大图片你可能会说不好看，接下来我们进行更换这个图片，首先我们先找到一个自己想更换的图片,接下来我们将他放到图床当中去，本人经常用路过图床,然后将我们的图片上传上去，这里会显示一个连接，我们先复制这个链接。 然后找到我们的主题配置文件(路径应该是 博客根目录/_config.butterfly.yml),然后找到default_top_img:这一行将链接替换成你刚才复制的链接即可. 说道这里我们顺便配置一下我们的头像，头像和首页图配置方法是一样的只要在配置文件中找到头像的位置将URL放入即可。 配置主页的导航栏我们可以看到我们的导航栏都是英文，我们可以自己手动修改。 我们打开我们的主题配置文档，路径在哪里我也就不用说了吧，在博客根目录/_config.butterfly.yml(最后一遍了)配置文件开头就是我们的导航栏配置，默认配置如下 12345678910menu: Home: &#x2F; || fa fa-home Archives: &#x2F;archives&#x2F; || fa fa-archive Tags: &#x2F;tags&#x2F; || fa fa-tags Categories: &#x2F;categories&#x2F; || fa fa-folder-open Link: &#x2F;link&#x2F; || fa fa-link About: &#x2F;about&#x2F; || fa fa-heart List||fa fa-list: - Music || &#x2F;music&#x2F; || fa fa-music - Movie || &#x2F;movies&#x2F; || fa fa-film 最终我修改完成后如下所示: 12345678910menu: 首页: &#x2F; || fas fa-home 归档: &#x2F;archives&#x2F; || fas fa-archive 标签: &#x2F;tags&#x2F; || fas fa-tags 分类: &#x2F;categories&#x2F; || fas fa-folder-open 清单||fas fa-list: - Music || &#x2F;music&#x2F; || fas fa-music - Movie || &#x2F;movies&#x2F; || fas fa-video 友情链接: &#x2F;link&#x2F; || fas fa-link 关于: &#x2F;about&#x2F; || fas fa-heart 相信聪明的你，一看就懂了 主页名人名言 我们在主题配置文件中搜索 Never put off till tomorrow what you can do today然后我们将这句话改成我不知道将去何方 但我已在路上最终实现了替换的效果，并且我们还可以去掉上边的一句话 我们还可以自动获取网络上的好句子，我们只需要将source: false换成下面你想选择的样式 source: 1 #调用博天API https://api.btstu.cn/ source: 2 #调用一言API https://hitokoto.cn/ source: 3 #调用一句话API http://yijuzhan.com/ source: 4 #调用今日詩詞API https://www.jinrishici.com/ 这里要注意的是: 在显示的时候，会先实现网络获取的一句话，然后在获取本地设置的话 更多butterfly美化在这哦 部署云端现在我们的博客，感觉已经很漂亮了，接下来我们部署云端让所有人看到我们的博客吧!!! 最后！我们需要额外的一个工具来帮助我们推到仓库上，那就是！那就是！那就是 hexo-deployer-git搞它！ 执行下面的命令，进行安装插件 1npm install hexo-deployer-git --save 使用GitHub部署云端 我们在Github中建立一个仓库，其中仓库名字必须是xxx.github.ioxxx是你的Github名字,过程如图所示 接下来，打开我们的博客配置文件,找到下面的内容 1234deploy: type: git repo: &lt;你的仓库地址&gt; # https:&#x2F;&#x2F;github.com&#x2F;TJ-XiaJiaHao&#x2F;TJ-XiaJiaHao.github.io branch: master 然后写上你刚刚建立仓库的地址 执行下面的命令，进行远程部署到我们的Github的仓库 1234 # 清理垃圾hexo clean# 推送到远端hexo d浏览器访问：https://xxxxx.github.io/ 即可看到效果。(把xxxx替换成的Github名字就可以了)例如我的是 https://ciks111.github.io/ 不要着急，可能会有1-2分钟的延迟 最终终于大工告成了 本文也就到此结束了.","categories":[{"name":"hexo","slug":"hexo","permalink":"http://example.com/categories/hexo/"}],"tags":[{"name":"hexo博客搭建","slug":"hexo博客搭建","permalink":"http://example.com/tags/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"author":"Charles"}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"hexo","slug":"hexo","permalink":"http://example.com/categories/hexo/"}],"tags":[{"name":"进入深度学习","slug":"进入深度学习","permalink":"http://example.com/tags/%E8%BF%9B%E5%85%A5%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"特征工程-特征降维","slug":"特征工程-特征降维","permalink":"http://example.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"},{"name":"特征工程-特征预处理","slug":"特征工程-特征预处理","permalink":"http://example.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/"},{"name":"数据处理与特征工程-特征提取","slug":"数据处理与特征工程-特征提取","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"},{"name":"了解机器学习","slug":"了解机器学习","permalink":"http://example.com/tags/%E4%BA%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"hexo博客搭建","slug":"hexo博客搭建","permalink":"http://example.com/tags/hexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}]}